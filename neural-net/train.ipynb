{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4725e347-6162-44b9-9c8f-f1906094af4d",
   "metadata": {},
   "source": [
    "# Chesser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e2d49-0725-4e6c-a8ed-dac14ec14944",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eab9d2c-722d-4503-8140-e9e0e3347a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "# Check Tensorflow identifies GPU and enable memory growth\n",
    "#physical_devices = tf.config.experimental.list_physical_Devices('GPU')\n",
    "#print(\"Num GPUs available: \" , len(physical_devices))\n",
    "#tf.config.experimental.set_memory_grows(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790876c1-f45e-4b5b-8181-caa92e063c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('input/classes')\n",
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee9274e-ce57-413f-9c5c-47fedfb6d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 994 files belonging to 13 classes.\n",
      "Using 796 files for training.\n",
      "Found 994 files belonging to 13 classes.\n",
      "Using 198 files for validation.\n",
      "13 ['BB', 'BK', 'BN', 'BP', 'BQ', 'BR', 'Empty', 'WB', 'WK', 'WN', 'WP', 'WQ', 'WR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:06.931484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:06.946799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:06.947457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:06.948697: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-12 07:55:06.949902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:06.950470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:06.950949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:08.039718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:08.040716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:08.041326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 07:55:08.041758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6120 MB memory:  -> device: 0, name: GRID V100S-8Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Train Dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  seed=123,\n",
    "  subset=\"training\",\n",
    "  label_mode='categorical',\n",
    "  #image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# Validation Dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  seed=123,\n",
    "  subset=\"validation\",\n",
    "  label_mode='categorical',\n",
    "  #image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(train_ds.class_names)\n",
    "print(num_classes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca92a03-79d3-44ad-ac53-f8f0343ad702",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bad7d24-e8ee-4147-9174-852a8504afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:08.247950: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12476595 0.8694392\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d303bbe-c257-42fb-9adf-a5e20bb87809",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df485093-19ce-46e4-88b7-65ba3d1b44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#def configure_for_performance(ds):\n",
    "#  ds = ds.cache()\n",
    "#  ds = ds.shuffle(buffer_size=1000)\n",
    "#  ds = ds.batch(batch_size)\n",
    "#  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "#  return ds\n",
    "\n",
    "#train_ds = configure_for_performance(train_ds)\n",
    "#val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0f07f-f577-431b-99af-759b3961aff4",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b394518-db10-4384-b8a4-539fad052dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:09.473194: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-12 07:55:09.473251: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-01-12 07:55:09.473314: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2022-01-12 07:55:09.782732: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-12 07:55:09.786116: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:11.942429: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/13 [=>............................] - ETA: 46s - loss: 13.9167 - accuracy: 0.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:13.726700: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-01-12 07:55:13.726753: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/13 [===>..........................] - ETA: 9s - loss: 90.1888 - accuracy: 0.0391 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:14.608809: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-01-12 07:55:14.612947: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2022-01-12 07:55:14.659189: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 334 callback api events and 331 activity events. \n",
      "2022-01-12 07:55:14.668317: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-01-12 07:55:14.687834: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2022_01_12_07_55_14\n",
      "\n",
      "2022-01-12 07:55:14.700120: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.trace.json.gz\n",
      "2022-01-12 07:55:14.734114: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/train/plugins/profile/2022_01_12_07_55_14\n",
      "\n",
      "2022-01-12 07:55:14.736223: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.memory_profile.json.gz\n",
      "2022-01-12 07:55:14.737149: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/train/plugins/profile/2022_01_12_07_55_14\n",
      "Dumped tool data for xplane.pb to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2022_01_12_07_55_14/jupyter-adb553.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 7s 234ms/step - loss: 22.8111 - accuracy: 0.0905 - val_loss: 0.4314 - val_accuracy: 0.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:16.848531: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3657891840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 0.4718 - accuracy: 0.1018 - val_loss: 0.3876 - val_accuracy: 0.1061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:25.040630: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3657891840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.4124 - accuracy: 0.1407 - val_loss: 0.3654 - val_accuracy: 0.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:33.399812: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3657891840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "13/13 [==============================] - 3s 131ms/step - loss: 0.3557 - accuracy: 0.1595 - val_loss: 0.3213 - val_accuracy: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:42.171518: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3657891840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.3293 - accuracy: 0.1683 - val_loss: 0.3028 - val_accuracy: 0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 07:55:50.575575: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3657891840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "13/13 [==============================] - 3s 127ms/step - loss: 0.2941 - accuracy: 0.2224 - val_loss: 0.2910 - val_accuracy: 0.1364\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "# architecture of the neural network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5,5),\n",
    "                activation='relu',\n",
    "                kernel_constraint=max_norm(3.)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#Conv block 2\n",
    "model.add(Conv2D(64, kernel_size=(5,5), activation='relu', kernel_constraint=max_norm(3.)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Identification\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', name='features'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(13, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='Adam',\n",
    "             metrics=['accuracy'])\n",
    "# the loss is minimized by the neural net\n",
    "# optimizer is the method for training (Adam= stochastic gradient descent, adaptive estimation of first-order and second-order moments)\n",
    "# metrics accuracy divides total by count, the number of correctly identified images to total number of images\n",
    "\n",
    "LOGDIR = \"./logs\"\n",
    "my_tensorboard = TensorBoard(log_dir = LOGDIR,\n",
    "                            histogram_freq=1,\n",
    "                            write_graph=True,\n",
    "                            write_images=True)\n",
    "# Hyperparamter\n",
    "my_epochs = 10\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                   batch_size=batch_size,\n",
    "                   callbacks=[my_tensorboard],\n",
    "                   epochs=my_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=val_ds)\n",
    "\n",
    "#model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
